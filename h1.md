# The Analysis of the Paper "The Weaponization of Artificial Intelligence in Cybersecurity: A Systematic Review"

_By F.Anashchenkov for the course Cybersecurity (ICB705AS3YE-3001)_

## Introduction

The paper ["The Weaponization of Artificial Intelligence in Cybersecurity: A Systematic Review"](https://www.sciencedirect.com/science/article/pii/S1877050924014492) written by C.Nobles and published in 2024 drew my attention because of the topic: the development and ongoing integration of AI into different processes and areas of life, and the consequences of these changes is one of the hottest topics in academia. "Weaponization" of AI, i.e. using advanced AI algorithms for cyber attacks, is merely a matter of time, as well as measures against such attacks. The paper summarized the existing literature on the topic.

In my opinion, the main idea of the paper is that AI is transforming cybersecurity into a technological battlefield or chess, where both attackers and defenders are using AI: one to find and exploit vulnerabilities, the other to detect and stop attacks. Success in mitigating attacks against AI-based systems or with the use of AI cyberweapons lie in combining forces between governments, society and business to not only work out solutions but to improve average digital literacy and education and set up efficient content moderation to fight disinformation.

## Research Questions

The paper asked three questions:

- Is there a connection between weaponized AI and cybersecurity countermeasures?
- What mitigation/protection strategies does the literature propose?
- How are AI-driven attacks changing cybersecurity?

## Methodology

Authors searched major databases such as Scopus, Springer, IEEE, Wiley, ACM, Google Scholar and selected works by certain inclusion and exclusion criteria. They featured peer-reviewed English works relevant to the questions. with stated methods and data that can be analyzed. They then applied the systematic review consisting of several stages (Identification, Screening, Eligibility, Inclusion). The final list for review had 21 articles, published between 2017 and 2023. Authors also categorized selected articles by their quality, from "Very Good" to "Poor".

## Key Findings

### New Threats

Authors found a number of similar topics discussed throughout analyzed literature, in particular 11 cybersecurity threats "born" as a result of AI weaponization. Here are these threats (with additional explanation):

1. **AI/ML contamination**. Manipulating training data to skew predictions, leading to false or malicious outcomes (e.g., false positives or negatives).
2. **AI-powered malware**. Using deep learning to make malware more autonomous, stealthy, and adaptive. Such malware can use various tactics to bypass traditional cybersecurity defense.
3. **AI-induced vulnerabilities**. Weak regulation, erosion of privacy, and overreliance on autonomous AI systems create systemic vulnerabilities, e.g., fragile cyber-physical systems and self-learning attacks.
4. **Weaponization of code**. Refers to the misuse of code control and deployment processes (especially in open-source and automated projects), leading to the mass creation and dissemination of cyberweapons.
5. **Weaponization of disinformation and social media**. AI gets used to automate and amplify disinformation campaigns, manipulate public opinion or destabilize societies.
6. **AI-driven attacks**. AI is used to plan and automate cyberattacks: it can find targets, infiltrate them, and stay hidden faster and more efficient than human hackers.
7. **Security threats to AI models**. AI models can be hacked in three main ways: (1) evasion (fake or altered input) so it starts to make wrong decisions; (2) poisoning (passing bad data into training sets); (3) model stealing (copying or cloning an AI model to make it unreliable or launch more targeted attacks).
8. **Weaponization of AI/ML**. Hackers are able to design AI systems specifically for attack instead of defence. This AI software can act independently, bypass traditional security and launch attacks without human intervention.
9. **Social engineering attacks**. Use of AI can enhance psychological manipulation by generating personalized phishing, deepfakes and fake identities.
10. **Network intrusion evasion**. AI-guided malware adapts to avoid detection by intrusion prevention systems, altering its behavior dynamically to remain hidden within network traffic.
11. **AI-based autonomy intelligence**. Hackers design systems with self-learning and decision-making abilities, making cyberweapons more autonomous, resilient, and difficult to trace or neutralize.

Thus, AI-related threats identified in the reviewed literature reveal AI's exceptional self-learning and decision-making abilities compared to conventional software, which makes cyberweapons more autonomous, resilient, and difficult to trace or neutralize. This is the reality cybersecurity specialists have to deal with.

### Defences and Strategies Against AI-Driven Attacks

By studying the literature, authors identified a clear link between the use of weaponized AI for attacks and the development of AI-based tools designed to defend against them. This relationship has created a "technological arms race", where both sides (cybersecurity teams and hackers) are continuously advancing their capabilities.

Organizations are increasingly using AI and machine learning to improve security monitoring, detect threats faster, analyze complex patterns, and respond to incidents in real time. These AI-based protections are capable of processing big amounts of data and anticipate possible attack paths before they occur. On the other hand, the paper warns that overreliance on automated systems can create new vulnerabilities: if defenders trust AI blindly, attackers can exploit its weaknesses or manipulate its data inputs. In the end, effective cybersecurity requires not only advanced AI tools but also human oversight, constant testing, and transparency in how models make decisions.

Based on the conducted analysis, the authors claim that society can reduce the risks of using AI for harmful purposes and increase protection against AI-driven cyberattacks.

To counter the new threats, there is a need for more collaboration among policymakers, engineers, researchers, and technology companies. Governments, social media platforms, and business should work together to create stronger detection systems, better content filters, and improved tools to stop the spread of false or manipulated information.

Media and digital literacy are also important: they can help individuals identify and resist disinformation and online manipulation caused by malicious AI.

Cybersecurity systems must continuously evolve, because attacks powered by AI use complex and hidden methods that can fool traditional tools. Defensive systems need to adapt and respond to new threats. Research into the dual role of AI should be intensified to better understand and anticipate risks in cybersecurity.

Global society should also promote transparency and ethical design in AI instead of blind trust and overreliance on AI-driven applications.

Finally, automation brought by AI should always be coupled with human oversight, ensuring that humans remain in control.

## Closing Thoughts

The analyzed paper draws a bleak picture based on existing literature. Hackers actively weaponize AI to make cyber attacks and malware even more efficient, while other malicious actors can adopt AI to create sophisticated fake content to manipulate entire societies.

Yet, there is a light at the end of a tunnel. The same technology that empowers attackers can also strengthen defenders. With ethical design, transparency, and appropriate regulation, AI can be used for creating sufficient protection against new threats. Collaboration between governments, industry, and researchers, combined with global efforts in education and awareness can help create a strong foundation and eliminate the malignant influence on AI.
